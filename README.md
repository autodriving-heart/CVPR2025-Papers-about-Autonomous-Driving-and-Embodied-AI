# CVPR 2025-Papers-about-Autonomous-Driving-and-Embodied-AI

## Autonomous-Driving

### (1) Occupancy

**UniScene: Unified Occupancy-centric Driving Scene Generation**

- paper: https://arxiv.org/pdf/2412.05435

- code: https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation
  
**GaussianFormer-2: Probabilistic Gaussian Superposition for Efficient 3D Occupancy Prediction**

- paper: https://arxiv.org/abs/2412.04384

- code: https://github.com/huang-yh/GaussianFormer

### (2) End-to-End Autonomous-Driving

**Don't Shake the Wheel: Momentum-AwarePlanning in End-to-End Autonomous Driving**

- paper: https://arxiv.org/pdf/2503.03125

- code: https://github.com/adept-thu/MomAD

**DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving**

- paper: https://arxiv.org/pdf/2411.15139

- code: https://github.com/hustvl/DiffusionDrive

### (3) Multi-modal Fusion

**V2X - R: Cooperative LiDAR - 4D Radar Fusion with Denoising Diffusion for 3D Object Detection**

- paper: https://arxiv.org/pdf/2411.08402

- code: https://github.com/ylwhxht/V2X-R

### (4) Vision Language Model

**OmniDrive: A Holistic Vision - Language Dataset for Autonomous Driving with counter Factual Reasoning**

- paper: https://arxiv.org/pdf/2405.01533

- code: https://github.com/NVlabs/OmniDrive

### (5) World Model

**DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene**

- paper: https://arxiv.org/pdf/2410.13571

- code: https://github.com/GigaAI-research/DriveDreamer4D

**ReconDreamer: Crafting World Models for Driving Scene Reconstruction via Online Restoration**

- paper: https://arxiv.org/pdf/2411.19548

- code: https://github.com/GigaAI-research/ReconDreamer

### (6) Closed Loop

**DrivingSphere: Building a High-fidelity 4D World for Closed-loop Simulation**

- paper: https://arxiv.org/abs/2411.11252

- code: https://github.com/yanty123/DrivingSphere

### (7) Lane Detection

**Rethinking Lanes and Points in complex Scenarios for Monocular 3D Lane Detection**

- paper: https://arxiv.org/abs/2503.06237

### (8) Motion Prediction

**ModeSeq: Taming Sparse Multimodal Motion Prediction with Sequential Mode Modeling**

- paper: https://arxiv.org/pdf/2411.11911

## Embodied-AI

### (1) Diffusion Model

**DexHandDiff: Interaction - aware Diffusion Planning for Adaptive Dexterous Manipulation**

- paper: https://arxiv.org/abs/2411.18562

### (2) 3D Semantic Flow

**G3Flow: Generative 3D Semantic Flow for Pose-aware and Generalizable Object Manipulation**

- paper: https://arxiv.org/abs/2411.18369
- code:  https://github.com/TianxingChen/G3Flow

### (3) Dual-Arm Robot

**RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins**

- paper: https://arxiv.org/abs/2409.02920
- code: https://github.com/TianxingChen/RoboTwin?tab=readme-ov-file#robotwin-dual-arm-robot-benchmark-with-generative-digital-twins

**Towards Long-Horizon Vision-Language Navigation: Platform, Benchmark and Method**

- paper: https://arxiv.org/pdf/2412.09082
- code: https://github.com/HCPLab-SYSU/LH-VLN
- 
### (4) Visual Language Model

**Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection**

- paper: https://arxiv.org/abs/2412.04455

**PhysVLM: Enabling Visual Language Models to Understand Robotic Physical Reachability**

--paper: https://arxiv.org/abs/2503.08481

## Others

**Mamba4D: Efficient 4D Point Cloud Video Understanding with Disentangled Spatial-Temporal State Space Models**

- paper: https://arxiv.org/abs/2405.14338

- code: https://github.com/IRMVLab/Mamba4D

**Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model**

- paper: https://arxiv.org/abs/2411.19108

- code: https://github.com/ali-vilab/TeaCache

**DynRefer: Delving into Region-level Multi-modality Tasks via Dynamic Resolution**

- paper: https://arxiv.org/abs/2405.16071

- code: https://github.com/callsys/DynRefer

**OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations**

- paper: https://arxiv.org/pdf/2412.07626

- code: https://arxiv.org/pdf/2412.07626

**VTON360: High-Fidelity Virtual Try-On from Any Viewing Direction**

- paper: https://arxiv.org/abs/2503.12165

- code: https://github.com/scnuhealthy/VTON360

**Cross-modal Causal Relation Alignment for Video Question Grounding**

-- paper: https://arxiv.org/abs/2503.07635

-- code: https://github.com/WissingChen/CRA-GQA

**DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering**

-- paper: https://arxiv.org/abs/2503.03190

-- code: https://github.com/LZ-CH/DSPNet

**Reproducible Vision-Language Models Meet Concepts Out of Pre-Training**

**LLM-driven Multimodal and Multi-Identity Listening Head Generation**

**DAGSM: Disentangled Avatar Generation with GS-enhanced Mesh**

-- paper: https://arxiv.org/abs/2411.15205

**HiRes-LLaVA: Restoring Fragmentation Input in High-Resolution Large Vision-Language Models**

-- paper: https://arxiv.org/abs/2407.08706

**FireEdit: Fine-grained Instruction-based Image Editing via Region-aware Vision Language Model**

-- paper: https://arxiv.org/abs/2503.19839

-- code: https://zjgans.github.io/fireedit.github.io/

**EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions**

-- paper: https://arxiv.org/abs/2409.18042

-- code: https://emova-anonymous.github.io/

**PS-Diffusion: Photorealistic Subject-Driven Image Editing with Disentangled Control and Attention**

**Boosting the Dual-Stream Architecture in Ultra-High Resolution Segmentation with Resolution-Biased Uncertainty Estimation**

**No Pains, More Gains: Recycling Sub-Salient Patches for Efficient High-Resolution Image Recognition**

**Empowering Large Language Models with 3D Situation Awareness**

-- paper: https://arxiv.org/abs/2503.23024

**Rethinking Query-based Transformer for Continual Image Segmentation**

**Eval3D: Interpretable and Fine-grained Evaluation for 3D Generation**

- paper: http://arxiv.org/pdf/2504.18509v1

- code: https://eval3d.git

**Seeing Soundscapes: Audio-Visual Generation and Separation from Soundscapes Using Audio-Visual Separator**

- paper: http://arxiv.org/pdf/2504.18283v1

**SSL4Eco: A Global Seasonal Dataset for Geospatial Foundation Models in Ecology**

- paper: http://arxiv.org/pdf/2504.18256v1

- code: https://github.com/PlekhanovaElena/ssl4eco

**BiasBench: A reproducible benchmark for tuning the biases of event cameras**

- paper: http://arxiv.org/pdf/2504.18235v1

**Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models**

- paper: http://arxiv.org/pdf/2504.18032v1

- code: https://chenchen-usyd.git

**Dynamic Camera Poses and Where to Find Them**

- paper: http://arxiv.org/pdf/2504.17788v1

- code: https://research.nvidia.com/labs/dir/dynpose-100k

**PICO: Reconstructing 3D People In Contact with Objects**

- paper: http://arxiv.org/pdf/2504.17695v1

- code: https://pico.is.tue.mpg.de

**VEU-Bench: Towards Comprehensive Understanding of Video Editing**

- paper: http://arxiv.org/pdf/2504.17828v1

**Dual Prompting Image Restoration with Diffusion Transformers**

- paper: http://arxiv.org/pdf/2504.17825v1

**SemanticSugarBeets: A Multi-Task Framework and Dataset for Inspecting Harvest and Storage Characteristics of Sugar Beets**

- paper: http://arxiv.org/pdf/2504.16684v1

- code: https://github.com/semanticsugarbeets/semanticsugarbeets

**PRaDA: Projective Radial Distortion Averaging**

- paper: http://arxiv.org/pdf/2504.16499v1

**CLOC: Contrastive Learning for Ordinal Classification with Multi-Margin N-pair Loss**

- paper: http://arxiv.org/pdf/2504.17813v1

**LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale**

- paper: http://arxiv.org/pdf/2504.16030v1

- code: https://showlab.git

**PointLoRA: Low-Rank Adaptation with Token Selection for Point Cloud Learning**

- paper: http://arxiv.org/pdf/2504.16023v1

- code: https://github.com/songw-zju/PointLoRA

**RepNet-VSR: Reparameterizable Architecture for High-Fidelity Video Super-Resolution**

- paper: http://arxiv.org/pdf/2504.15649v1

**MirrorVerse: Pushing Diffusion Models to Realistically Reflect the World**

- paper: http://arxiv.org/pdf/2504.15397v1

- code: https://mirror-verse.git

**Plug-and-Play Versatile Compressed Video Enhancement**

- paper: http://arxiv.org/pdf/2504.15380v1

- code: https://huimin-zeng.git

**Acquire and then Adapt: Squeezing out Text-to-Image Model for Image Restoration**

- paper: http://arxiv.org/pdf/2504.15159v1

**Improving Sound Source Localization with Joint Slot Attention on Image and Audio**

- paper: http://arxiv.org/pdf/2504.15118v1

**NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement: KwaiSR Dataset and Study**

- paper: http://arxiv.org/pdf/2504.15003v1

- code: https://lixinustc.git

**DyFo: A Training-Free Dynamic Focus Visual Search for Enhancing LMMs in Fine-Grained Visual Understanding**

- paper: http://arxiv.org/pdf/2504.14920v1

- code: https://github.com/PKU-ICST-MIPL/DyFo_CVPR2025

## Other Unfiled papers

**LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale**

- paper: https://arxiv.org/pdf/2504.16030

**An LLM-enabled Multi-Agent Autonomous Mechatronics Design Framework**

- paper: https://arxiv.org/pdf/2504.14681

**CheXWorld: Exploring Image World Modeling for Radiograph Representation Learning**

- paper: https://arxiv.org/pdf/2504.13820

**EchoWorld: Learning Motion-Aware World Models for Echocardiography Probe Guidance**

- paper: https://arxiv.org/pdf/2504.13065

**RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins**

- paper: https://arxiv.org/pdf/2504.13059

**Rethinking Temporal Fusion with a Unified Gradient Descent View for 3D Semantic Occupancy Prediction**

- paper: https://arxiv.org/pdf/2504.12959

**ZeroGrasp: Zero-Shot Shape Reconstruction Enabled Robotic Grasping**

- paper: https://arxiv.org/pdf/2504.10857

**CleanMAP: Distilling Multimodal LLMs for Confidence-Driven Crowdsourced HD Map Updates**

- paper: https://arxiv.org/pdf/2504.10738

**Foundation Models for Remote Sensing: An Analysis of MLLMs for Object Localization**

- paper: https://arxiv.org/pdf/2504.10727

**SilVar-Med: A Speech-Driven Visual Language Model for Explainable Abnormality Detection in Medical Imaging**

- paper: https://arxiv.org/pdf/2504.10642

**Ges3ViG: Incorporating Pointing Gestures into Language-Based 3D Visual Grounding for Embodied Reference Understanding**

- paper: https://arxiv.org/pdf/2504.09623

**Gen3DEval: Using vLLMs for Automatic Evaluation of Generated 3D Objects**

- paper: https://arxiv.org/pdf/2504.08125

**MESA: Text-Driven Terrain Generation Using Latent Diffusion and Global Copernicus Data**

- paper: https://arxiv.org/pdf/2504.07210

**Neural Motion Simulator: Pushing the Limit of World Models in Reinforcement Learning**

- paper: https://arxiv.org/pdf/2504.07095

**Two by Two: Learning Multi-Task Pairwise Objects Assembly for Generalizable Robot Manipulation**

- paper: https://arxiv.org/pdf/2504.06961

**From Broadcast to Minimap: Achieving State-of-the-Art SoccerNet Game State Reconstruction**

- paper: https://arxiv.org/pdf/2504.06357

**OCC-MLLM-CoT-Alpha: Towards Multi-stage Occlusion Recognition Based on Large Language Models via 3D-Aware Supervision and Chain-of-Thoughts Guidance**

- paper: https://arxiv.org/pdf/2504.04781

**Data Scaling Laws for End-to-End Autonomous Driving**

- paper: https://arxiv.org/pdf/2504.04338

**Decision SpikeFormer: Spike-Driven Transformer for Decision Making**

- paper: https://arxiv.org/pdf/2504.03800

**Shape My Moves: Text-Driven Shape-Aware Synthesis of Human Motions**

- paper: https://arxiv.org/pdf/2504.03639

**ZFusion: An Effective Fuser of Camera and 4D Radar for 3D Object Perception in Autonomous Driving**

- paper: https://arxiv.org/pdf/2504.03438

**AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent Framework for Resume Screening**

- paper: https://arxiv.org/pdf/2504.02870

**Exploration-Driven Generative Interactive Environments**

- paper: https://arxiv.org/pdf/2504.02515

**Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation**

- paper: https://arxiv.org/pdf/2504.00420

**MPDrive: Improving Spatial Understanding with Marker-Based Prompt Learning for Autonomous Driving**

- paper: https://arxiv.org/pdf/2504.00379

**Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs**

- paper: https://arxiv.org/pdf/2504.00072

**Scenario Dreamer: Vectorized Latent Diffusion for Generating Driving Simulation Environments**

- paper: https://arxiv.org/pdf/2503.22496

**Protecting Your Video Content: Disrupting Automated Video-based LLM Annotations**

- paper: https://arxiv.org/pdf/2503.21824

**CoMapGS: Covisibility Map-based Gaussian Splatting for Sparse Novel View Synthesis**

- paper: https://arxiv.org/pdf/2503.20998

**BioX-CPath: Biologically-driven Explainable Diagnostics for Multistain IHC Computational Pathology**

- paper: https://arxiv.org/pdf/2503.20880

**CoLLM: A Large Language Model for Composed Image Retrieval**

- paper: https://arxiv.org/pdf/2503.19910

**Attention IoU: Examining Biases in CelebA using Attention Maps**

- paper: https://arxiv.org/pdf/2503.19846

**AudCast: Audio-Driven Human Video Generation by Cascaded Diffusion Transformers**

- paper: https://arxiv.org/pdf/2503.19824

**Teller: Real-Time Streaming Audio-Driven Portrait Animation with Autoregressive Motion Generation**

- paper: https://arxiv.org/pdf/2503.18429

**HiLoTs: High-Low Temporal Sensitive Representation Learning for Semi-Supervised LiDAR Segmentation in Autonomous Driving**

- paper: https://arxiv.org/pdf/2503.17752

**CountLLM: Towards Generalizable Repetitive Action Counting via Large Language Model**

- paper: https://arxiv.org/pdf/2503.17690

**Missing Target-Relevant Information Prediction with World Model for Accurate Zero-Shot Composed Image Retrieval**

- paper: https://arxiv.org/pdf/2503.17109

**Vision-Language Gradient Descent-driven All-in-One Deep Unfolding Networks**

- paper: https://arxiv.org/pdf/2503.16930

**MASH-VLM: Mitigating Action-Scene Hallucination in Video-LLMs through Disentangled Spatial-Temporal Representations**

- paper: https://arxiv.org/pdf/2503.15871

**Generating Multimodal Driving Scenes via Next-Scene Prediction**

- paper: https://arxiv.org/pdf/2503.14945

**When Domain Generalization meets Generalized Category Discovery: An Adaptive Task-Arithmetic Driven Approach**

- paper: https://arxiv.org/pdf/2503.14897

**Bridging Past and Future: End-to-End Autonomous Driving with Historical Prediction and Planning**

- paper: https://arxiv.org/pdf/2503.14182

**MP-GUI: Modality Perception with MLLMs for GUI Understanding**

- paper: https://arxiv.org/pdf/2503.14021

**DIFFVSGG: Diffusion-Driven Online Video Scene Graph Generation**

- paper: https://arxiv.org/pdf/2503.13957

**Conformal Prediction and MLLM aided Uncertainty Quantification in Scene Graph Generation**

- paper: https://arxiv.org/pdf/2503.13947

**SALAD: Skeleton-aware Latent Diffusion for Text-driven Motion Generation and Editing**

- paper: https://arxiv.org/pdf/2503.13836

**Omnia de EgoTempo: Benchmarking Temporal Understanding of Multi-Modal LLMs in Egocentric Videos**

- paper: https://arxiv.org/pdf/2503.13646

**Efficient Motion-Aware Video MLLM**

- paper: https://arxiv.org/pdf/2503.13016

**V-Stylist: Video Stylization via Collaboration and Reflection of MLLM Agents**

- paper: https://arxiv.org/pdf/2503.12077

**Minding Fuzzy Regions: A Data-driven Alternating Learning Paradigm for Stable Lesion Segmentation**

- paper: https://arxiv.org/pdf/2503.11140

**DriveGEN: Generalized and Robust 3D Detection in Driving via Controllable Text-to-Image Diffusion Generation**

- paper: https://arxiv.org/pdf/2503.11122

**Spatial-Temporal Graph Diffusion Policy with Kinematic Modeling for Bimanual Robotic Manipulation**

- paper: https://arxiv.org/pdf/2503.10743

**GaussHDR: High Dynamic Range Gaussian Splatting via Learning Unified 3D and 2D Local Tone Mapping**

- paper: https://arxiv.org/pdf/2503.10143

**SimLingo: Vision-Only Closed-Loop Autonomous Driving with Language-Action Alignment**

- paper: https://arxiv.org/pdf/2503.09594

**Out-of-Distribution Segmentation in Autonomous Driving: Problems and State of the Art**

- paper: https://arxiv.org/pdf/2503.08695

**DexGrasp Anything: Towards Universal Robotic Dexterous Grasping with Physics Awareness**

- paper: https://arxiv.org/pdf/2503.08257

**VidBot: Learning Generalizable 3D Actions from In-the-Wild 2D Human Videos for Zero-Shot Robotic Manipulation**

- paper: https://arxiv.org/pdf/2503.07135

**A Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning**

- paper: https://arxiv.org/pdf/2503.06960

**Denoising Functional Maps: Diffusion Models for Shape Correspondence**

- paper: https://arxiv.org/pdf/2503.01845

**KeyFace: Expressive Audio-Driven Facial Animation for Long Sequences via KeyFrame Interpolation**

- paper: https://arxiv.org/pdf/2503.01715

**Order-Robust Class Incremental Learning: Graph-Driven Dynamic Similarity Grouping**

- paper: https://arxiv.org/pdf/2502.20032

**CarPlanner: Consistent Auto-regressive Trajectory Planning for Large-scale Reinforcement Learning in Autonomous Driving**

- paper: https://arxiv.org/pdf/2502.19908

**Transfer Your Perspective: Controllable 3D Generation from Any Viewpoint in a Driving Scene**

- paper: https://arxiv.org/pdf/2502.06682

**Universal Actions for Enhanced Embodied Foundation Models**

- paper: https://arxiv.org/pdf/2501.10105

**OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?**

- paper: https://arxiv.org/pdf/2501.05510

**MotionMap: Representing Multimodality in Human Pose Forecasting**

- paper: https://arxiv.org/pdf/2412.18883

**Adapter Merging with Centroid Prototype Mapping for Scalable Class-Incremental Learning**

- paper: https://arxiv.org/pdf/2412.18219

**GME: Improving Universal Multimodal Retrieval by Multimodal LLMs**

- paper: https://arxiv.org/pdf/2412.16855

**Empowering LLMs to Understand and Generate Complex Vector Graphics**

- paper: https://arxiv.org/pdf/2412.11102

**StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements**

- paper: https://arxiv.org/pdf/2412.08503

**DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation**

- paper: https://arxiv.org/pdf/2412.07589

**UniScene: Unified Occupancy-centric Driving Scene Generation**

- paper: https://arxiv.org/pdf/2412.05435

**DualPM: Dual Posed-Canonical Point Maps for 3D Shape and Pose Reconstruction**

- paper: https://arxiv.org/pdf/2412.04464

**Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection**

- paper: https://arxiv.org/pdf/2412.04455

**Navigation World Models**

- paper: https://arxiv.org/pdf/2412.03572

**Task-driven Image Fusion with Learnable Fusion Loss**

- paper: https://arxiv.org/pdf/2412.03240

**UniGraspTransformer: Simplified Policy Distillation for Scalable Dexterous Robotic Grasping**

- paper: https://arxiv.org/pdf/2412.02699

**Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding**

- paper: https://arxiv.org/pdf/2412.00493

**Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via Inference-Time Alignment**

- paper: https://arxiv.org/pdf/2411.18688

**CityWalker: Learning Embodied Urban Navigation from Web-Scale Videos**

- paper: https://arxiv.org/pdf/2411.17820

**SAMWISE: Infusing Wisdom in SAM2 for Text-Driven Video Segmentation**

- paper: https://arxiv.org/pdf/2411.17646

**Augmenting Multimodal LLMs with Self-Reflective Tokens for Knowledge-based Visual Question Answering**

- paper: https://arxiv.org/pdf/2411.16863

**RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics**

- paper: https://arxiv.org/pdf/2411.16537

**DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving**

- paper: https://arxiv.org/pdf/2411.15139

**Voxel-Aggregated Feature Synthesis: Efficient Dense Mapping for Simulated 3D Reasoning**

- paper: https://arxiv.org/pdf/2411.10616

**Decoupling Fine Detail and Global Geometry for Compressed Depth Map Super-Resolution**

- paper: https://arxiv.org/pdf/2411.03239

**Driving by the Rules: A Benchmark for Integrating Traffic Sign Regulations into Vectorized HD Map**

- paper: https://arxiv.org/pdf/2410.23780

**Style-Editor: Text-driven object-centric style editing**

- paper: https://arxiv.org/pdf/2408.08461

**3D-MVP: 3D Multiview Pretraining for Robotic Manipulation**

- paper: https://arxiv.org/pdf/2406.18158

**Mitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation**

- paper: https://arxiv.org/pdf/2406.14235

**3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination**

- paper: https://arxiv.org/pdf/2406.05132

**VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos**

- paper: https://arxiv.org/pdf/2405.19209
